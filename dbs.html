<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Databases</title>
</head>
<body>
</style><link rel="stylesheet" href="style.css">

<h1 style="text-align:center">P.SAMD</h1>

<!-- Horizontal menu -->
<div class="navbar">
    <a href="index.html">Home</a>
    <a href="dbs.html" class="active">Databases</a>
    <a href="model.html">Model</a>
    <a href="training.html">Training</a>
    <a href="performance.html">Performance</a>
</div>

<h1 style="text-align:center">Databases</h1>

<!-- Table od databases -->
<table border="1" style="border-collapse: collapse; margin: 0 auto">
    <thead>
        <tr>
            <th>Name</th>
            <th>Count</th>
            <th>Dimensions</th>
            <th>Usage</th>
            <th>Description</th>
            <th>Reference</th>
            <th>Source Audio</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>NISQA_TRAIN_SIM</td>
            <td>10,000</td>
            <td>overall, noi, dis, col, loud</td>
            <td>Train</td>
            <td>Contains simulated distortions with speech samples from four different datasets. Divided into a training and a validation set.</td>
            <td>Where were the ratings from? P.808? P.800? POLQA?</td>
            <td></td>
        </tr>
        <tr>
            <td>NISQA_TRAIN_LIVE</td>
            <td>1,020</td>
            <td>overall, noi, dis, col, loud</td>
            <td>Train</td>
            <td>Contains live phone and Skype recordings with Librivox audiobook samples. Divided into training and validation set.</td>
            <td></td>
            <td></td>
        </tr>
        <tr>
            <td>NISQA_VAL_SIM</td>
            <td>2,500</td>
            <td>overall, noi, dis, col, loud</td>
            <td>Validation</td>
            <td>contains simulated distortions with speech samples from four different datasets. Divided into a training and a validation set.</td>
            <td></td>
            <td></td>
        </tr>
        <tr>
            <td>NISQA_VAL_LIVE</td>
            <td>200</td>
            <td>overall, noi, dis, col, loud</td>
            <td>Validation</td>
            <td>Contains live phone and Skype recordings with Librivox audiobook samples. Divided into training and validation set.</td>
            <td></td>
            <td></td>
        </tr>
        <tr>
            <td>NISQA_TEST_LIVETALK</td>
            <td>232</td>
            <td>overall, noi, dis, col, loud</td>
            <td>Test</td>
            <td>Contains simulated distortions with different codecs, background noises, packet-loss, clipping. It also contains live conditions with WhatsApp, Zoom, and Discord.</td>
            <td></td>
            <td></td>
        </tr>
        <tr>
            <td>NISQA_TEST_FOR</td>
            <td>240</td>
            <td>overall, noi, dis, col, loud</td>
            <td>Test</td>
            <td>Contains live and simulated conditions with speech samples from the forensic speech dataset.</td>
            <td>Crowdsourced according to ITU-T P.808</td>
            <td>Source speech samples are taken from the "Forensic Voice Comparison Databases - Australian English: 500+ speakers" dataset.</td>
        </tr>
        <tr>
            <td>NISQA_TEST_NSC</td>
            <td>240</td>
            <td>overall, noi, dis, col, loud</td>
            <td>Test</td>
            <td>Contains simulated distortions with different codecs, background noises, packet-loss, clipping. It also contains live conditions with Skype, Zoom, and Google Meet, and mobile-to-landline.</td>
            <td>Crowdsourced according to ITU-T P.808</td>
            <td>Source speech samples are taken from the "Nautilus Speaker Characterization (NSC) Corpus" database.</td>
        </tr>
        <tr>
            <td>NISQA_TEST_P501</td>
            <td>240</td>
            <td>overall, noi, dis, col, loud</td>
            <td>Test</td>
            <td>Contains simulated distortions with different codecs, background noises, packet-loss, clipping. It also contains live conditions with Skype, Zoom, WhatsApp, and mobile network recordings.</td>
            <td>Crowdsourced according to ITU-T P.808</td>
            <td>Source speech samples are taken from Annex C of the "ITU-T P.501" dataset</td>
        </tr>
    </tbody>
</table>

<div>
    <br> 
    <strong>Notes on Gabriel's DBs</strong>
    <p>Of all the databases Gabriel used to train NISQA2.0: 16,241 samples contain all 5 scores (overall quality plus
         all 4 quality dimension scores).</p>
</div>

<div>
    <h2>New Databases</h2>

    <p><strong>TUB-PSAMD-current</strong><br> 
        This is a new database being created via Advanced Projects at the QU Lab Winter semsester 2023. Source speech files are yet 
        to be selected. Annotation will be carried out in the lab following P.800 (Wafaa W.). Conditions are to be 
        defined. Expected date to completion is end of January 2024.</p>

    <p><strong>TUB-WP7-current</strong><br>
        This is part of another project at the QU Lab (Kirill S. & Wafaa W.) where source files are crowdsourced in various live 
        conditions. The Crowdee platform is used for collecting the source audio. About 2000 samples are expected. These will undergo 
        screening, then simulated degradations wi√∂ll be applied to selected samples. The speech files will ttthen be annotated 
        via the same crowdsourcing platform. The database will then be used to train/finetune NISQA (and/or newer models) to check the 
        suitability of crowdsourced source files in Speech Quality Assessment. Expected date to completion is end of January 2024. 
    </p>

</div>

<div>
    <h2>Links to resources:</h2>
    <a href="https://github.com/gabrielmittag/NISQA/wiki/NISQA-Corpus">NISQA Corpus</a>

</div>



</body>
</html>