<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Model</title>
</head>
<body>
</style><link rel="stylesheet" href="style.css">

<h1 style="text-align:center">P.SAMD</h1>

<!-- Horizontal menu -->
<div class="navbar">
    <a href="index.html">Home</a>
    <a href="reqspecs.html">Requirements Specifications</a>
    <a href="dbs.html">Databases</a>
    <a href="model.html" class="active">Model</a>
    <a href="training.html">Training</a>
    <a href="performance.html">Performance</a>
</div>

<h1 style="text-align:center">Model</h1>

<h2><u>Upstream model</u></h2>

<div>
    <p><strong>Pipeline for XLS-R53 feature extraction</strong></p>
        <p>First, a pretrained model of the Wav2Vec2 model from the Facebook AI repository is initialized 
            using the Hugging Face Transformers library. There are three modules in the Wav2Vec2Model: 
            feature_extractor, feature_projector, and encoder. The speech/audio features can be extracted at 
            any of those modules in the architecture sequence. This is something that can be explored 
            in the overall model. First, only the feature_extractor module output is used in the P.SAMD 
            upstream model. Later, the other two modules can be experimented with to see if performance improves.
            the Wav2Vec2FeatureExtractor to preprocess the audio and then passes it through the Wav2Vec2Model to extract 
            feature vectors directly from the audio</p>
</div>

<h3>XLS-R Feature Extractor</h3>
<p>The paper <a href="https://arxiv.org/abs/2006.13979">Unsupervised Cross-lingual Representation Learning for Speech Recognition</a> describes 
    the XLS-R model and its feauture extractor.</p>

<p>Wave2Vec 2.0, including the XLS-R variant, extracts features from raw audio waveforms using a deep neural network architecture based on a 
    self-supervised learning approach. The model processes the raw audio waveforms to learn meaningful representations directly from the audio signal. 
    Here's how Wave2Vec 2.0 typically extracts features from WAV files:</p>

<ol>
    <li>Feature Extraction with Convolutional Neural Networks (CNNs): <br>
        Wave2Vec 2.0 employs a series of convolutional layers to process the raw audio waveforms. The convolutional neural network operates on small 
        chunks of the waveform, capturing local patterns in the audio signal.
    </li>
    <li>Contrastive Learning (Self-Supervised Learning)</li>
    <li>Strided Convolutional Layers, with Pooling and Aggregation</li>
    <li>Hierarchical and Final Feature Representations</li>
</ol>

<p>The pretrained feature extractor of Wave2Vec 2.0 can handle audio of varying duration during inference. While the model was trained on fixed-length 
    segments of 800 milliseconds with a 400 ms overlap, this does not restrict the model's ability to process audio signals of different durations 
    during the inference phase.
</p>

<p>Layer output processing: whether to pool and reduce the final tensor size or keep all the layer vectors stacked without losing any layer information.
    3 verrsions of the feature extractor can be explored:</p>

<ul>
    <li>No layer aggregation</li>
    <li>Average-pooling the layers</li>
    <li>Max-pooling the layers</li>
</ul>

<p>Disadvantage: downsampling audio from 48kbps to 16kbps can alter the high-frequency content, spectral balance, introduce artifacts, and 
    impact psychoacoustic cues, all of which can lead to changes in the perceived coloration of the audio. </p>

<h3>Mel-Spectrogram</h3>
<p>TBD</p>

<h3>MFCC</h3>
<p>TBD</p>

<h2><u>Downstream model</u></h2>

<h2><u>Development variants</u></h2>

<ol>
    <li>
        <strong>V1:</strong> <br>
        <u>Upstream:</u> XLS-R feature extractor (all 7 CNN layer feature vectors) This is from the PyTorch pipeline.<br>
        Input shape: waveform -> Feature extraction -> torch.Size([7, 449, 1024]) -> slicing -> torch.Size([7, 200, 1024]) -> flattening -> torch.Size([1433600]). <br>
        <u>Downstream:</u> Linear regressor with simple topology: Linear(1433600, 512) -> ReLU() -> Linear(512, 256) -> ReLU() -> Linear(256, 5) <br>
    </li> <br>
    <li>
        <strong>V2:</strong> <br>
        <u>Upstream:</u> XLS-R feature extractor (all 7 CNN layer feature vectors) This is from the Hugging Face Transformers library.<br>
        Input shape: waveform -> Feature extraction ->  <br>
        <u>Downstream:</u> Linear regressor with simple topology: Linear(1433600, 512) -> ReLU() -> Linear(512, 256) -> ReLU() -> Linear(256, 5) <br>
    </li>
</ol>

</body>
</html>